{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "# from util import load_dataset, read_sensor_data_from_file\n",
    "from resnet1d import ResNet1D, MyDataset\n",
    "from acnn1d import ACNN, MyDataset\n",
    "from crnn1d import CRNN, MyDataset\n",
    "from net1d import Net1D, MyDataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "def read_sensor_data_from_file(file_path):\n",
    "    \"\"\"\n",
    "    读取txt文件中的数据，跳过第一行并只读取第2行到第127000行的数据（假设数据格式为127000*8）。\n",
    "    \"\"\"\n",
    "    # data = pd.read_csv(file_path, delimiter=',',skiprows=1).values\n",
    "    data = np.loadtxt(file_path)\n",
    "\n",
    "    # if data.shape[0] < 126999 or data.shape[1] != 8:\n",
    "    #     raise ValueError(f\"File {file_path} has unexpected shape {data.shape}\")\n",
    "\n",
    "    # trimmed_data = data[0:126998, :]  # 从第2行到第127000行的数据\n",
    "\n",
    "    # return trimmed_data\n",
    "    return data\n",
    "\n",
    "def load_dataset(data_folder):\n",
    "    \"\"\"\n",
    "    从指定的文件夹加载数据集。数据文件在多层目录中，标签由文件夹名称确定。\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_Y = []\n",
    "    label_map = {'Eucalyptus': 0, 'Rosewood': 1, 'Sandalwood': 2}\n",
    "\n",
    "    for subfolder_name in os.listdir(data_folder):\n",
    "        subfolder_path = os.path.join(data_folder, subfolder_name)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "        \n",
    "        if subfolder_name not in label_map:\n",
    "            print(f\"Unknown label folder: {subfolder_name}\")\n",
    "            continue\n",
    "        \n",
    "        label = label_map[subfolder_name]\n",
    "        \n",
    "        for root, dirs, files in os.walk(subfolder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        data = read_sensor_data_from_file(file_path)\n",
    "                        if data.shape == (1269, 8):# 126998\n",
    "                            data = np.transpose(data, (1, 0))\n",
    "                            all_X.append(data)\n",
    "                            all_Y.append(label)\n",
    "                        else:\n",
    "                            print(f\"File {file_path} has unexpected shape {data.shape}\")\n",
    "                    except ValueError as e:\n",
    "                        print(e)\n",
    "\n",
    "    all_X = np.array(all_X)\n",
    "    all_Y = np.array(all_Y)\n",
    "    return all_X, all_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 8, 1269) Counter({0: 900, 1: 900, 2: 900})\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'C:/Users/22209/Desktop/气体论文/Woodgass/Normal'\n",
    "data, label = load_dataset(data_folder)\n",
    "print(data.shape, Counter(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into train, validation, and test sets (8:1:1)\n",
    "# data_train, data_temp, label_train, label_temp = train_test_split(data, label, test_size=0.2, random_state=42, stratify=label)\n",
    "# data_val, data_test, label_val, label_test = train_test_split(data_temp, label_temp, test_size=0.5, random_state=42, stratify=label_temp)\n",
    "\n",
    "# # Create datasets and dataloaders\n",
    "# train_dataset = MyDataset(data_train, label_train)\n",
    "# val_dataset = MyDataset(data_val, label_val)\n",
    "# test_dataset = MyDataset(data_test, label_test)  \n",
    "\n",
    "# 只划分训练和验证就是8；2\n",
    "# split data into training and validation sets\n",
    "data_train, data_val, label_train, label_val = train_test_split(data, label, test_size=0.2, random_state=42, stratify=label)\n",
    "\n",
    "# create datasets and dataloaders\n",
    "train_dataset = MyDataset(data_train, label_train)\n",
    "val_dataset = MyDataset(data_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False) # 只划分train和val的时候就不需要划分test了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 128, 1269]           9,344\n",
      "   MyConv1dPadSame-2            [-1, 128, 1269]               0\n",
      "       BatchNorm1d-3            [-1, 128, 1269]             256\n",
      "              ReLU-4            [-1, 128, 1269]               0\n",
      "            Conv1d-5            [-1, 128, 1269]          18,560\n",
      "   MyConv1dPadSame-6            [-1, 128, 1269]               0\n",
      "       BatchNorm1d-7            [-1, 128, 1269]             256\n",
      "              ReLU-8            [-1, 128, 1269]               0\n",
      "           Dropout-9            [-1, 128, 1269]               0\n",
      "           Conv1d-10            [-1, 128, 1269]          18,560\n",
      "  MyConv1dPadSame-11            [-1, 128, 1269]               0\n",
      "       BasicBlock-12            [-1, 128, 1269]               0\n",
      "      BatchNorm1d-13            [-1, 128, 1269]             256\n",
      "             ReLU-14            [-1, 128, 1269]               0\n",
      "          Dropout-15            [-1, 128, 1269]               0\n",
      "           Conv1d-16             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-17             [-1, 128, 635]               0\n",
      "      BatchNorm1d-18             [-1, 128, 635]             256\n",
      "             ReLU-19             [-1, 128, 635]               0\n",
      "          Dropout-20             [-1, 128, 635]               0\n",
      "           Conv1d-21             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-22             [-1, 128, 635]               0\n",
      "        MaxPool1d-23             [-1, 128, 635]               0\n",
      "MyMaxPool1dPadSame-24             [-1, 128, 635]               0\n",
      "       BasicBlock-25             [-1, 128, 635]               0\n",
      "      BatchNorm1d-26             [-1, 128, 635]             256\n",
      "             ReLU-27             [-1, 128, 635]               0\n",
      "          Dropout-28             [-1, 128, 635]               0\n",
      "           Conv1d-29             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-30             [-1, 128, 635]               0\n",
      "      BatchNorm1d-31             [-1, 128, 635]             256\n",
      "             ReLU-32             [-1, 128, 635]               0\n",
      "          Dropout-33             [-1, 128, 635]               0\n",
      "           Conv1d-34             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-35             [-1, 128, 635]               0\n",
      "       BasicBlock-36             [-1, 128, 635]               0\n",
      "      BatchNorm1d-37             [-1, 128, 635]             256\n",
      "             ReLU-38             [-1, 128, 635]               0\n",
      "          Dropout-39             [-1, 128, 635]               0\n",
      "           Conv1d-40             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-41             [-1, 128, 635]               0\n",
      "      BatchNorm1d-42             [-1, 128, 635]             256\n",
      "             ReLU-43             [-1, 128, 635]               0\n",
      "          Dropout-44             [-1, 128, 635]               0\n",
      "           Conv1d-45             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-46             [-1, 128, 635]               0\n",
      "       BasicBlock-47             [-1, 128, 635]               0\n",
      "      BatchNorm1d-48             [-1, 128, 635]             256\n",
      "             ReLU-49             [-1, 128, 635]               0\n",
      "          Dropout-50             [-1, 128, 635]               0\n",
      "           Conv1d-51             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-52             [-1, 128, 635]               0\n",
      "      BatchNorm1d-53             [-1, 128, 635]             256\n",
      "             ReLU-54             [-1, 128, 635]               0\n",
      "          Dropout-55             [-1, 128, 635]               0\n",
      "           Conv1d-56             [-1, 128, 635]          18,560\n",
      "  MyConv1dPadSame-57             [-1, 128, 635]               0\n",
      "       BasicBlock-58             [-1, 128, 635]               0\n",
      "      BatchNorm1d-59             [-1, 128, 635]             256\n",
      "             ReLU-60             [-1, 128, 635]               0\n",
      "          Dropout-61             [-1, 128, 635]               0\n",
      "           Conv1d-62             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-63             [-1, 128, 318]               0\n",
      "      BatchNorm1d-64             [-1, 128, 318]             256\n",
      "             ReLU-65             [-1, 128, 318]               0\n",
      "          Dropout-66             [-1, 128, 318]               0\n",
      "           Conv1d-67             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-68             [-1, 128, 318]               0\n",
      "        MaxPool1d-69             [-1, 128, 318]               0\n",
      "MyMaxPool1dPadSame-70             [-1, 128, 318]               0\n",
      "       BasicBlock-71             [-1, 128, 318]               0\n",
      "      BatchNorm1d-72             [-1, 128, 318]             256\n",
      "             ReLU-73             [-1, 128, 318]               0\n",
      "          Dropout-74             [-1, 128, 318]               0\n",
      "           Conv1d-75             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-76             [-1, 128, 318]               0\n",
      "      BatchNorm1d-77             [-1, 128, 318]             256\n",
      "             ReLU-78             [-1, 128, 318]               0\n",
      "          Dropout-79             [-1, 128, 318]               0\n",
      "           Conv1d-80             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-81             [-1, 128, 318]               0\n",
      "       BasicBlock-82             [-1, 128, 318]               0\n",
      "      BatchNorm1d-83             [-1, 128, 318]             256\n",
      "             ReLU-84             [-1, 128, 318]               0\n",
      "          Dropout-85             [-1, 128, 318]               0\n",
      "           Conv1d-86             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-87             [-1, 128, 318]               0\n",
      "      BatchNorm1d-88             [-1, 128, 318]             256\n",
      "             ReLU-89             [-1, 128, 318]               0\n",
      "          Dropout-90             [-1, 128, 318]               0\n",
      "           Conv1d-91             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-92             [-1, 128, 318]               0\n",
      "       BasicBlock-93             [-1, 128, 318]               0\n",
      "      BatchNorm1d-94             [-1, 128, 318]             256\n",
      "             ReLU-95             [-1, 128, 318]               0\n",
      "          Dropout-96             [-1, 128, 318]               0\n",
      "           Conv1d-97             [-1, 128, 318]          18,560\n",
      "  MyConv1dPadSame-98             [-1, 128, 318]               0\n",
      "      BatchNorm1d-99             [-1, 128, 318]             256\n",
      "            ReLU-100             [-1, 128, 318]               0\n",
      "         Dropout-101             [-1, 128, 318]               0\n",
      "          Conv1d-102             [-1, 128, 318]          18,560\n",
      " MyConv1dPadSame-103             [-1, 128, 318]               0\n",
      "      BasicBlock-104             [-1, 128, 318]               0\n",
      "     BatchNorm1d-105             [-1, 128, 318]             256\n",
      "            ReLU-106             [-1, 128, 318]               0\n",
      "         Dropout-107             [-1, 128, 318]               0\n",
      "          Conv1d-108             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-109             [-1, 128, 159]               0\n",
      "     BatchNorm1d-110             [-1, 128, 159]             256\n",
      "            ReLU-111             [-1, 128, 159]               0\n",
      "         Dropout-112             [-1, 128, 159]               0\n",
      "          Conv1d-113             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-114             [-1, 128, 159]               0\n",
      "       MaxPool1d-115             [-1, 128, 159]               0\n",
      "MyMaxPool1dPadSame-116             [-1, 128, 159]               0\n",
      "      BasicBlock-117             [-1, 128, 159]               0\n",
      "     BatchNorm1d-118             [-1, 128, 159]             256\n",
      "            ReLU-119             [-1, 128, 159]               0\n",
      "         Dropout-120             [-1, 128, 159]               0\n",
      "          Conv1d-121             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-122             [-1, 128, 159]               0\n",
      "     BatchNorm1d-123             [-1, 128, 159]             256\n",
      "            ReLU-124             [-1, 128, 159]               0\n",
      "         Dropout-125             [-1, 128, 159]               0\n",
      "          Conv1d-126             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-127             [-1, 128, 159]               0\n",
      "      BasicBlock-128             [-1, 128, 159]               0\n",
      "     BatchNorm1d-129             [-1, 128, 159]             256\n",
      "            ReLU-130             [-1, 128, 159]               0\n",
      "         Dropout-131             [-1, 128, 159]               0\n",
      "          Conv1d-132             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-133             [-1, 128, 159]               0\n",
      "     BatchNorm1d-134             [-1, 128, 159]             256\n",
      "            ReLU-135             [-1, 128, 159]               0\n",
      "         Dropout-136             [-1, 128, 159]               0\n",
      "          Conv1d-137             [-1, 128, 159]          18,560\n",
      " MyConv1dPadSame-138             [-1, 128, 159]               0\n",
      "      BasicBlock-139             [-1, 128, 159]               0\n",
      "     BatchNorm1d-140             [-1, 128, 159]             256\n",
      "            ReLU-141             [-1, 128, 159]               0\n",
      "         Dropout-142             [-1, 128, 159]               0\n",
      "          Conv1d-143             [-1, 256, 159]          37,120\n",
      " MyConv1dPadSame-144             [-1, 256, 159]               0\n",
      "     BatchNorm1d-145             [-1, 256, 159]             512\n",
      "            ReLU-146             [-1, 256, 159]               0\n",
      "         Dropout-147             [-1, 256, 159]               0\n",
      "          Conv1d-148             [-1, 256, 159]          73,984\n",
      " MyConv1dPadSame-149             [-1, 256, 159]               0\n",
      "      BasicBlock-150             [-1, 256, 159]               0\n",
      "     BatchNorm1d-151             [-1, 256, 159]             512\n",
      "            ReLU-152             [-1, 256, 159]               0\n",
      "         Dropout-153             [-1, 256, 159]               0\n",
      "          Conv1d-154              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-155              [-1, 256, 80]               0\n",
      "     BatchNorm1d-156              [-1, 256, 80]             512\n",
      "            ReLU-157              [-1, 256, 80]               0\n",
      "         Dropout-158              [-1, 256, 80]               0\n",
      "          Conv1d-159              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-160              [-1, 256, 80]               0\n",
      "       MaxPool1d-161              [-1, 256, 80]               0\n",
      "MyMaxPool1dPadSame-162              [-1, 256, 80]               0\n",
      "      BasicBlock-163              [-1, 256, 80]               0\n",
      "     BatchNorm1d-164              [-1, 256, 80]             512\n",
      "            ReLU-165              [-1, 256, 80]               0\n",
      "         Dropout-166              [-1, 256, 80]               0\n",
      "          Conv1d-167              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-168              [-1, 256, 80]               0\n",
      "     BatchNorm1d-169              [-1, 256, 80]             512\n",
      "            ReLU-170              [-1, 256, 80]               0\n",
      "         Dropout-171              [-1, 256, 80]               0\n",
      "          Conv1d-172              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-173              [-1, 256, 80]               0\n",
      "      BasicBlock-174              [-1, 256, 80]               0\n",
      "     BatchNorm1d-175              [-1, 256, 80]             512\n",
      "            ReLU-176              [-1, 256, 80]               0\n",
      "         Dropout-177              [-1, 256, 80]               0\n",
      "          Conv1d-178              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-179              [-1, 256, 80]               0\n",
      "     BatchNorm1d-180              [-1, 256, 80]             512\n",
      "            ReLU-181              [-1, 256, 80]               0\n",
      "         Dropout-182              [-1, 256, 80]               0\n",
      "          Conv1d-183              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-184              [-1, 256, 80]               0\n",
      "      BasicBlock-185              [-1, 256, 80]               0\n",
      "     BatchNorm1d-186              [-1, 256, 80]             512\n",
      "            ReLU-187              [-1, 256, 80]               0\n",
      "         Dropout-188              [-1, 256, 80]               0\n",
      "          Conv1d-189              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-190              [-1, 256, 80]               0\n",
      "     BatchNorm1d-191              [-1, 256, 80]             512\n",
      "            ReLU-192              [-1, 256, 80]               0\n",
      "         Dropout-193              [-1, 256, 80]               0\n",
      "          Conv1d-194              [-1, 256, 80]          73,984\n",
      " MyConv1dPadSame-195              [-1, 256, 80]               0\n",
      "      BasicBlock-196              [-1, 256, 80]               0\n",
      "     BatchNorm1d-197              [-1, 256, 80]             512\n",
      "            ReLU-198              [-1, 256, 80]               0\n",
      "         Dropout-199              [-1, 256, 80]               0\n",
      "          Conv1d-200              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-201              [-1, 256, 40]               0\n",
      "     BatchNorm1d-202              [-1, 256, 40]             512\n",
      "            ReLU-203              [-1, 256, 40]               0\n",
      "         Dropout-204              [-1, 256, 40]               0\n",
      "          Conv1d-205              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-206              [-1, 256, 40]               0\n",
      "       MaxPool1d-207              [-1, 256, 40]               0\n",
      "MyMaxPool1dPadSame-208              [-1, 256, 40]               0\n",
      "      BasicBlock-209              [-1, 256, 40]               0\n",
      "     BatchNorm1d-210              [-1, 256, 40]             512\n",
      "            ReLU-211              [-1, 256, 40]               0\n",
      "         Dropout-212              [-1, 256, 40]               0\n",
      "          Conv1d-213              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-214              [-1, 256, 40]               0\n",
      "     BatchNorm1d-215              [-1, 256, 40]             512\n",
      "            ReLU-216              [-1, 256, 40]               0\n",
      "         Dropout-217              [-1, 256, 40]               0\n",
      "          Conv1d-218              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-219              [-1, 256, 40]               0\n",
      "      BasicBlock-220              [-1, 256, 40]               0\n",
      "     BatchNorm1d-221              [-1, 256, 40]             512\n",
      "            ReLU-222              [-1, 256, 40]               0\n",
      "         Dropout-223              [-1, 256, 40]               0\n",
      "          Conv1d-224              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-225              [-1, 256, 40]               0\n",
      "     BatchNorm1d-226              [-1, 256, 40]             512\n",
      "            ReLU-227              [-1, 256, 40]               0\n",
      "         Dropout-228              [-1, 256, 40]               0\n",
      "          Conv1d-229              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-230              [-1, 256, 40]               0\n",
      "      BasicBlock-231              [-1, 256, 40]               0\n",
      "     BatchNorm1d-232              [-1, 256, 40]             512\n",
      "            ReLU-233              [-1, 256, 40]               0\n",
      "         Dropout-234              [-1, 256, 40]               0\n",
      "          Conv1d-235              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-236              [-1, 256, 40]               0\n",
      "     BatchNorm1d-237              [-1, 256, 40]             512\n",
      "            ReLU-238              [-1, 256, 40]               0\n",
      "         Dropout-239              [-1, 256, 40]               0\n",
      "          Conv1d-240              [-1, 256, 40]          73,984\n",
      " MyConv1dPadSame-241              [-1, 256, 40]               0\n",
      "      BasicBlock-242              [-1, 256, 40]               0\n",
      "     BatchNorm1d-243              [-1, 256, 40]             512\n",
      "            ReLU-244              [-1, 256, 40]               0\n",
      "         Dropout-245              [-1, 256, 40]               0\n",
      "          Conv1d-246              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-247              [-1, 256, 20]               0\n",
      "     BatchNorm1d-248              [-1, 256, 20]             512\n",
      "            ReLU-249              [-1, 256, 20]               0\n",
      "         Dropout-250              [-1, 256, 20]               0\n",
      "          Conv1d-251              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-252              [-1, 256, 20]               0\n",
      "       MaxPool1d-253              [-1, 256, 20]               0\n",
      "MyMaxPool1dPadSame-254              [-1, 256, 20]               0\n",
      "      BasicBlock-255              [-1, 256, 20]               0\n",
      "     BatchNorm1d-256              [-1, 256, 20]             512\n",
      "            ReLU-257              [-1, 256, 20]               0\n",
      "         Dropout-258              [-1, 256, 20]               0\n",
      "          Conv1d-259              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-260              [-1, 256, 20]               0\n",
      "     BatchNorm1d-261              [-1, 256, 20]             512\n",
      "            ReLU-262              [-1, 256, 20]               0\n",
      "         Dropout-263              [-1, 256, 20]               0\n",
      "          Conv1d-264              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-265              [-1, 256, 20]               0\n",
      "      BasicBlock-266              [-1, 256, 20]               0\n",
      "     BatchNorm1d-267              [-1, 256, 20]             512\n",
      "            ReLU-268              [-1, 256, 20]               0\n",
      "         Dropout-269              [-1, 256, 20]               0\n",
      "          Conv1d-270              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-271              [-1, 256, 20]               0\n",
      "     BatchNorm1d-272              [-1, 256, 20]             512\n",
      "            ReLU-273              [-1, 256, 20]               0\n",
      "         Dropout-274              [-1, 256, 20]               0\n",
      "          Conv1d-275              [-1, 256, 20]          73,984\n",
      " MyConv1dPadSame-276              [-1, 256, 20]               0\n",
      "      BasicBlock-277              [-1, 256, 20]               0\n",
      "     BatchNorm1d-278              [-1, 256, 20]             512\n",
      "            ReLU-279              [-1, 256, 20]               0\n",
      "         Dropout-280              [-1, 256, 20]               0\n",
      "          Conv1d-281              [-1, 512, 20]         147,968\n",
      " MyConv1dPadSame-282              [-1, 512, 20]               0\n",
      "     BatchNorm1d-283              [-1, 512, 20]           1,024\n",
      "            ReLU-284              [-1, 512, 20]               0\n",
      "         Dropout-285              [-1, 512, 20]               0\n",
      "          Conv1d-286              [-1, 512, 20]         295,424\n",
      " MyConv1dPadSame-287              [-1, 512, 20]               0\n",
      "      BasicBlock-288              [-1, 512, 20]               0\n",
      "     BatchNorm1d-289              [-1, 512, 20]           1,024\n",
      "            ReLU-290              [-1, 512, 20]               0\n",
      "         Dropout-291              [-1, 512, 20]               0\n",
      "          Conv1d-292              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-293              [-1, 512, 10]               0\n",
      "     BatchNorm1d-294              [-1, 512, 10]           1,024\n",
      "            ReLU-295              [-1, 512, 10]               0\n",
      "         Dropout-296              [-1, 512, 10]               0\n",
      "          Conv1d-297              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-298              [-1, 512, 10]               0\n",
      "       MaxPool1d-299              [-1, 512, 10]               0\n",
      "MyMaxPool1dPadSame-300              [-1, 512, 10]               0\n",
      "      BasicBlock-301              [-1, 512, 10]               0\n",
      "     BatchNorm1d-302              [-1, 512, 10]           1,024\n",
      "            ReLU-303              [-1, 512, 10]               0\n",
      "         Dropout-304              [-1, 512, 10]               0\n",
      "          Conv1d-305              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-306              [-1, 512, 10]               0\n",
      "     BatchNorm1d-307              [-1, 512, 10]           1,024\n",
      "            ReLU-308              [-1, 512, 10]               0\n",
      "         Dropout-309              [-1, 512, 10]               0\n",
      "          Conv1d-310              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-311              [-1, 512, 10]               0\n",
      "      BasicBlock-312              [-1, 512, 10]               0\n",
      "     BatchNorm1d-313              [-1, 512, 10]           1,024\n",
      "            ReLU-314              [-1, 512, 10]               0\n",
      "         Dropout-315              [-1, 512, 10]               0\n",
      "          Conv1d-316              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-317              [-1, 512, 10]               0\n",
      "     BatchNorm1d-318              [-1, 512, 10]           1,024\n",
      "            ReLU-319              [-1, 512, 10]               0\n",
      "         Dropout-320              [-1, 512, 10]               0\n",
      "          Conv1d-321              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-322              [-1, 512, 10]               0\n",
      "      BasicBlock-323              [-1, 512, 10]               0\n",
      "     BatchNorm1d-324              [-1, 512, 10]           1,024\n",
      "            ReLU-325              [-1, 512, 10]               0\n",
      "         Dropout-326              [-1, 512, 10]               0\n",
      "          Conv1d-327              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-328              [-1, 512, 10]               0\n",
      "     BatchNorm1d-329              [-1, 512, 10]           1,024\n",
      "            ReLU-330              [-1, 512, 10]               0\n",
      "         Dropout-331              [-1, 512, 10]               0\n",
      "          Conv1d-332              [-1, 512, 10]         295,424\n",
      " MyConv1dPadSame-333              [-1, 512, 10]               0\n",
      "      BasicBlock-334              [-1, 512, 10]               0\n",
      "     BatchNorm1d-335              [-1, 512, 10]           1,024\n",
      "            ReLU-336              [-1, 512, 10]               0\n",
      "         Dropout-337              [-1, 512, 10]               0\n",
      "          Conv1d-338               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-339               [-1, 512, 5]               0\n",
      "     BatchNorm1d-340               [-1, 512, 5]           1,024\n",
      "            ReLU-341               [-1, 512, 5]               0\n",
      "         Dropout-342               [-1, 512, 5]               0\n",
      "          Conv1d-343               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-344               [-1, 512, 5]               0\n",
      "       MaxPool1d-345               [-1, 512, 5]               0\n",
      "MyMaxPool1dPadSame-346               [-1, 512, 5]               0\n",
      "      BasicBlock-347               [-1, 512, 5]               0\n",
      "     BatchNorm1d-348               [-1, 512, 5]           1,024\n",
      "            ReLU-349               [-1, 512, 5]               0\n",
      "         Dropout-350               [-1, 512, 5]               0\n",
      "          Conv1d-351               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-352               [-1, 512, 5]               0\n",
      "     BatchNorm1d-353               [-1, 512, 5]           1,024\n",
      "            ReLU-354               [-1, 512, 5]               0\n",
      "         Dropout-355               [-1, 512, 5]               0\n",
      "          Conv1d-356               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-357               [-1, 512, 5]               0\n",
      "      BasicBlock-358               [-1, 512, 5]               0\n",
      "     BatchNorm1d-359               [-1, 512, 5]           1,024\n",
      "            ReLU-360               [-1, 512, 5]               0\n",
      "         Dropout-361               [-1, 512, 5]               0\n",
      "          Conv1d-362               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-363               [-1, 512, 5]               0\n",
      "     BatchNorm1d-364               [-1, 512, 5]           1,024\n",
      "            ReLU-365               [-1, 512, 5]               0\n",
      "         Dropout-366               [-1, 512, 5]               0\n",
      "          Conv1d-367               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-368               [-1, 512, 5]               0\n",
      "      BasicBlock-369               [-1, 512, 5]               0\n",
      "     BatchNorm1d-370               [-1, 512, 5]           1,024\n",
      "            ReLU-371               [-1, 512, 5]               0\n",
      "         Dropout-372               [-1, 512, 5]               0\n",
      "          Conv1d-373               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-374               [-1, 512, 5]               0\n",
      "     BatchNorm1d-375               [-1, 512, 5]           1,024\n",
      "            ReLU-376               [-1, 512, 5]               0\n",
      "         Dropout-377               [-1, 512, 5]               0\n",
      "          Conv1d-378               [-1, 512, 5]         295,424\n",
      " MyConv1dPadSame-379               [-1, 512, 5]               0\n",
      "      BasicBlock-380               [-1, 512, 5]               0\n",
      "     BatchNorm1d-381               [-1, 512, 5]           1,024\n",
      "            ReLU-382               [-1, 512, 5]               0\n",
      "         Dropout-383               [-1, 512, 5]               0\n",
      "          Conv1d-384               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-385               [-1, 512, 3]               0\n",
      "     BatchNorm1d-386               [-1, 512, 3]           1,024\n",
      "            ReLU-387               [-1, 512, 3]               0\n",
      "         Dropout-388               [-1, 512, 3]               0\n",
      "          Conv1d-389               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-390               [-1, 512, 3]               0\n",
      "       MaxPool1d-391               [-1, 512, 3]               0\n",
      "MyMaxPool1dPadSame-392               [-1, 512, 3]               0\n",
      "      BasicBlock-393               [-1, 512, 3]               0\n",
      "     BatchNorm1d-394               [-1, 512, 3]           1,024\n",
      "            ReLU-395               [-1, 512, 3]               0\n",
      "         Dropout-396               [-1, 512, 3]               0\n",
      "          Conv1d-397               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-398               [-1, 512, 3]               0\n",
      "     BatchNorm1d-399               [-1, 512, 3]           1,024\n",
      "            ReLU-400               [-1, 512, 3]               0\n",
      "         Dropout-401               [-1, 512, 3]               0\n",
      "          Conv1d-402               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-403               [-1, 512, 3]               0\n",
      "      BasicBlock-404               [-1, 512, 3]               0\n",
      "     BatchNorm1d-405               [-1, 512, 3]           1,024\n",
      "            ReLU-406               [-1, 512, 3]               0\n",
      "         Dropout-407               [-1, 512, 3]               0\n",
      "          Conv1d-408               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-409               [-1, 512, 3]               0\n",
      "     BatchNorm1d-410               [-1, 512, 3]           1,024\n",
      "            ReLU-411               [-1, 512, 3]               0\n",
      "         Dropout-412               [-1, 512, 3]               0\n",
      "          Conv1d-413               [-1, 512, 3]         295,424\n",
      " MyConv1dPadSame-414               [-1, 512, 3]               0\n",
      "      BasicBlock-415               [-1, 512, 3]               0\n",
      "     BatchNorm1d-416               [-1, 512, 3]           1,024\n",
      "            ReLU-417               [-1, 512, 3]               0\n",
      "         Dropout-418               [-1, 512, 3]               0\n",
      "          Conv1d-419              [-1, 1024, 3]         590,848\n",
      " MyConv1dPadSame-420              [-1, 1024, 3]               0\n",
      "     BatchNorm1d-421              [-1, 1024, 3]           2,048\n",
      "            ReLU-422              [-1, 1024, 3]               0\n",
      "         Dropout-423              [-1, 1024, 3]               0\n",
      "          Conv1d-424              [-1, 1024, 3]       1,180,672\n",
      " MyConv1dPadSame-425              [-1, 1024, 3]               0\n",
      "      BasicBlock-426              [-1, 1024, 3]               0\n",
      "     BatchNorm1d-427              [-1, 1024, 3]           2,048\n",
      "            ReLU-428              [-1, 1024, 3]               0\n",
      "         Dropout-429              [-1, 1024, 3]               0\n",
      "          Conv1d-430              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-431              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-432              [-1, 1024, 2]           2,048\n",
      "            ReLU-433              [-1, 1024, 2]               0\n",
      "         Dropout-434              [-1, 1024, 2]               0\n",
      "          Conv1d-435              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-436              [-1, 1024, 2]               0\n",
      "       MaxPool1d-437              [-1, 1024, 2]               0\n",
      "MyMaxPool1dPadSame-438              [-1, 1024, 2]               0\n",
      "      BasicBlock-439              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-440              [-1, 1024, 2]           2,048\n",
      "            ReLU-441              [-1, 1024, 2]               0\n",
      "         Dropout-442              [-1, 1024, 2]               0\n",
      "          Conv1d-443              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-444              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-445              [-1, 1024, 2]           2,048\n",
      "            ReLU-446              [-1, 1024, 2]               0\n",
      "         Dropout-447              [-1, 1024, 2]               0\n",
      "          Conv1d-448              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-449              [-1, 1024, 2]               0\n",
      "      BasicBlock-450              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-451              [-1, 1024, 2]           2,048\n",
      "            ReLU-452              [-1, 1024, 2]               0\n",
      "         Dropout-453              [-1, 1024, 2]               0\n",
      "          Conv1d-454              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-455              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-456              [-1, 1024, 2]           2,048\n",
      "            ReLU-457              [-1, 1024, 2]               0\n",
      "         Dropout-458              [-1, 1024, 2]               0\n",
      "          Conv1d-459              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-460              [-1, 1024, 2]               0\n",
      "      BasicBlock-461              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-462              [-1, 1024, 2]           2,048\n",
      "            ReLU-463              [-1, 1024, 2]               0\n",
      "         Dropout-464              [-1, 1024, 2]               0\n",
      "          Conv1d-465              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-466              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-467              [-1, 1024, 2]           2,048\n",
      "            ReLU-468              [-1, 1024, 2]               0\n",
      "         Dropout-469              [-1, 1024, 2]               0\n",
      "          Conv1d-470              [-1, 1024, 2]       1,180,672\n",
      " MyConv1dPadSame-471              [-1, 1024, 2]               0\n",
      "      BasicBlock-472              [-1, 1024, 2]               0\n",
      "     BatchNorm1d-473              [-1, 1024, 2]           2,048\n",
      "            ReLU-474              [-1, 1024, 2]               0\n",
      "         Dropout-475              [-1, 1024, 2]               0\n",
      "          Conv1d-476              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-477              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-478              [-1, 1024, 1]           2,048\n",
      "            ReLU-479              [-1, 1024, 1]               0\n",
      "         Dropout-480              [-1, 1024, 1]               0\n",
      "          Conv1d-481              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-482              [-1, 1024, 1]               0\n",
      "       MaxPool1d-483              [-1, 1024, 1]               0\n",
      "MyMaxPool1dPadSame-484              [-1, 1024, 1]               0\n",
      "      BasicBlock-485              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-486              [-1, 1024, 1]           2,048\n",
      "            ReLU-487              [-1, 1024, 1]               0\n",
      "         Dropout-488              [-1, 1024, 1]               0\n",
      "          Conv1d-489              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-490              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-491              [-1, 1024, 1]           2,048\n",
      "            ReLU-492              [-1, 1024, 1]               0\n",
      "         Dropout-493              [-1, 1024, 1]               0\n",
      "          Conv1d-494              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-495              [-1, 1024, 1]               0\n",
      "      BasicBlock-496              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-497              [-1, 1024, 1]           2,048\n",
      "            ReLU-498              [-1, 1024, 1]               0\n",
      "         Dropout-499              [-1, 1024, 1]               0\n",
      "          Conv1d-500              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-501              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-502              [-1, 1024, 1]           2,048\n",
      "            ReLU-503              [-1, 1024, 1]               0\n",
      "         Dropout-504              [-1, 1024, 1]               0\n",
      "          Conv1d-505              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-506              [-1, 1024, 1]               0\n",
      "      BasicBlock-507              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-508              [-1, 1024, 1]           2,048\n",
      "            ReLU-509              [-1, 1024, 1]               0\n",
      "         Dropout-510              [-1, 1024, 1]               0\n",
      "          Conv1d-511              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-512              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-513              [-1, 1024, 1]           2,048\n",
      "            ReLU-514              [-1, 1024, 1]               0\n",
      "         Dropout-515              [-1, 1024, 1]               0\n",
      "          Conv1d-516              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-517              [-1, 1024, 1]               0\n",
      "      BasicBlock-518              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-519              [-1, 1024, 1]           2,048\n",
      "            ReLU-520              [-1, 1024, 1]               0\n",
      "         Dropout-521              [-1, 1024, 1]               0\n",
      "          Conv1d-522              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-523              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-524              [-1, 1024, 1]           2,048\n",
      "            ReLU-525              [-1, 1024, 1]               0\n",
      "         Dropout-526              [-1, 1024, 1]               0\n",
      "          Conv1d-527              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-528              [-1, 1024, 1]               0\n",
      "       MaxPool1d-529              [-1, 1024, 1]               0\n",
      "MyMaxPool1dPadSame-530              [-1, 1024, 1]               0\n",
      "      BasicBlock-531              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-532              [-1, 1024, 1]           2,048\n",
      "            ReLU-533              [-1, 1024, 1]               0\n",
      "         Dropout-534              [-1, 1024, 1]               0\n",
      "          Conv1d-535              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-536              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-537              [-1, 1024, 1]           2,048\n",
      "            ReLU-538              [-1, 1024, 1]               0\n",
      "         Dropout-539              [-1, 1024, 1]               0\n",
      "          Conv1d-540              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-541              [-1, 1024, 1]               0\n",
      "      BasicBlock-542              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-543              [-1, 1024, 1]           2,048\n",
      "            ReLU-544              [-1, 1024, 1]               0\n",
      "         Dropout-545              [-1, 1024, 1]               0\n",
      "          Conv1d-546              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-547              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-548              [-1, 1024, 1]           2,048\n",
      "            ReLU-549              [-1, 1024, 1]               0\n",
      "         Dropout-550              [-1, 1024, 1]               0\n",
      "          Conv1d-551              [-1, 1024, 1]       1,180,672\n",
      " MyConv1dPadSame-552              [-1, 1024, 1]               0\n",
      "      BasicBlock-553              [-1, 1024, 1]               0\n",
      "     BatchNorm1d-554              [-1, 1024, 1]           2,048\n",
      "            ReLU-555              [-1, 1024, 1]               0\n",
      "          Linear-556                    [-1, 3]           3,075\n",
      "================================================================\n",
      "Total params: 36,978,051\n",
      "Trainable params: 36,978,051\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 87.96\n",
      "Params size (MB): 141.06\n",
      "Estimated Total Size (MB): 229.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# make model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# ----------------resnet------------------------\n",
    "model = ResNet1D(\n",
    "    in_channels=8, \n",
    "    base_filters=128,  #64\n",
    "    kernel_size=9,  \n",
    "    stride=2, \n",
    "    n_block=48, #34\n",
    "    groups=8,\n",
    "    n_classes=3, \n",
    "    downsample_gap=4, \n",
    "    increasefilter_gap=12, \n",
    "    verbose=False)\n",
    "model.to(device)\n",
    "summary(model, (data.shape[1], data.shape[2]))\n",
    "\n",
    "#----------------ACNN(Attention)-----------------\n",
    "# # make model\n",
    "# device_str = \"cuda\"\n",
    "# device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ACNN(\n",
    "#     in_channels=8, \n",
    "#     out_channels=128, \n",
    "#     att_channels=16,\n",
    "#     n_len_seg=32, \n",
    "#     verbose=True,\n",
    "#     n_classes=3,\n",
    "#     device=device)\n",
    "# model.to(device)\n",
    "\n",
    "# ## look model\n",
    "# prog_iter = tqdm(train_dataloader, desc=\"init\", leave=False)\n",
    "# for batch_idx, batch in enumerate(prog_iter):\n",
    "#     input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "#     pred = model(input_x)\n",
    "#     break\n",
    "\n",
    "#----------------DNN-----------------------------\n",
    "# model = Net1D(\n",
    "#     in_channels=1, \n",
    "#     base_filters=256, \n",
    "#     ratio=1.0, \n",
    "#     filter_mul_list=[1,2,2,4,4,8,8], \n",
    "#     m_blocks_list=[2,2,2,2,2,2,2], \n",
    "#     kernel_size=16, \n",
    "#     stride=2, \n",
    "#     groups=32,\n",
    "#     verbose=True, \n",
    "#     n_classes=4)\n",
    "# model.to(device)\n",
    "# summary(model, (X_train.shape[1], X_train.shape[2]), device=device_str)\n",
    "# exit()\n",
    "\n",
    "#----------------TCNN(Transformer)-----------------\n",
    "# device_str = \"cuda\"\n",
    "# device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CRNN(\n",
    "#     in_channels=1, \n",
    "#     out_channels=16, \n",
    "#     n_len_seg=50, \n",
    "#     verbose=False,\n",
    "#     n_classes=2,\n",
    "#     device=device)\n",
    "\n",
    "# summary(model, torch.zeros(1, 1, 3000))\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "#----------------CRNN(LSTM)------------------------\n",
    "# device_str = \"cuda\"\n",
    "# device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CRNN(\n",
    "#     in_channels=1, \n",
    "#     out_channels=16, \n",
    "#     n_len_seg=50, \n",
    "#     verbose=False,\n",
    "#     n_classes=2,\n",
    "#     device=device)\n",
    "\n",
    "# summary(model, torch.zeros(1, 1, 3000))\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input n_length should divided by n_len_seg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m input_x, input_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(input_x)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(pred, input_y)\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Ananconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ananconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\22209\\Desktop\\气体论文\\Woodgass\\resnet1d\\acnn1d.py:71\u001b[0m, in \u001b[0;36mACNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_channel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_length \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_len_seg \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput n_length should divided by n_len_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_seg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_len_seg\n\u001b[0;32m     74\u001b[0m     out \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mAssertionError\u001b[0m: Input n_length should divided by n_len_seg"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Initialize optimizer, loss function, and learning rate scheduler\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=10, verbose=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6, verbose=True)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(logdir='resnet1d/logs/resnet')\n",
    "\n",
    "# Training loop with TensorBoard logging and model saving\n",
    "\n",
    "# output_dir='./output/resnet'\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "        input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_x)\n",
    "        loss = loss_func(pred, input_y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    train_loss = np.mean(train_losses)\n",
    "    all_train_loss.append(train_loss)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_pred_prob = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "            pred = model(input_x)\n",
    "            loss = loss_func(pred, input_y)\n",
    "            val_loss += loss.item()\n",
    "            all_pred_prob.append(pred.cpu().data.numpy())\n",
    "            all_labels.append(input_y.cpu().data.numpy())\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    all_val_loss.append(val_loss)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping and saving the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 20:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Plotting\n",
    "plt.plot(all_train_loss, label='Training Loss')\n",
    "plt.plot(np.linspace(0, len(all_train_loss), len(all_val_loss)), all_val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8241\n",
      "Precision: 0.8348\n",
      "Recall: 0.8241\n",
      "F1 Score: 0.8259\n",
      "Confusion Matrix:\n",
      "[[162  17   1]\n",
      " [ 10 150  20]\n",
      " [  1  46 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       180\n",
      "           1       0.70      0.83      0.76       180\n",
      "           2       0.86      0.74      0.80       180\n",
      "\n",
      "    accuracy                           0.82       540\n",
      "   macro avg       0.83      0.82      0.83       540\n",
      "weighted avg       0.83      0.82      0.83       540\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.8240740740740741,\n",
       " 0.8347593002400461,\n",
       " 0.8240740740740741,\n",
       " 0.8258709965834825)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_pred_prob = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "            pred = model(input_x)\n",
    "            all_pred_prob.append(pred.cpu().data.numpy())\n",
    "            all_labels.append(input_y.cpu().data.numpy())\n",
    "    \n",
    "    all_pred_prob = np.concatenate(all_pred_prob)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_pred = np.argmax(all_pred_prob, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_pred)\n",
    "    precision = precision_score(all_labels, all_pred, average='weighted')\n",
    "    recall = recall_score(all_labels, all_pred, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_pred, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "# 如果想要使用验证集进行评估，原则上也只要把test换成val 应该就可以了\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "evaluate_model(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
